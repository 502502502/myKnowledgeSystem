### 1、用threadlocal做了什么，为什么要用，底层实现

> 做了什么

我有这样一个需求，就是在整个请求的处理过程中，会多次需要到当前用户对象，所以我想在请求处理之前，从数据库把这个对象加载到内存中，交给spring容器，需要的地方直接用注解注入就行，不用每次都创建。



> 为什么要用

一开始我想用session存储，使用session的好处是，同一个用户发的请求，不用每一次都去创建用户对象，直接从session获取就行，不同用户发的请求都从自己的session获取，实现并发隔离。而且使用redis替代session后，在分布式环境下，无论哪个结点来处理请求，都能直接获取创建好的用户对象。



但是有一个问题就是，用户对象是可变的，如果进行了用户资料的修改，需要同步到session或者redis，这就比较麻烦了，同步过程还得解决丢失，并发覆盖等问题。而且用户连续发送的多个请求，可能会出现多个线程同时操作同一个session的情况。这就导致一个请求在处理的过程中，它的用户对象是可能会变的，这个会留下一些隐患。



所以我用threadlocal来解决这个问题，去保证每一个线程执行过程中，它的用户对象信息都不会发生修改。在请求处理之前，使用拦截器从数据库加载对象，放到threadlocal中，整个处理过程使用的都是同一个对象，请求处理结束后手动从threadlocal删除用户对象。



> 实现原理

threadlocal有一个静态内部类ThreadLocalMap，这个Map是实现线程隔离的核心，它是自定义的，没有实现Map接口，用了一个Entry数组来存储threadlocal, value键值对，这个key是threadlocal实例，value是绑定的对象。

每个线程都有自己Map对象，用来存储自己线程的所有thtreadlocal对象和它绑定的对象，所以不会存在并发安全问题。

Map是local类的静态内部类，意味着，同样的线程，即使你有多个threadlocal实例，线程都只有一个Map实例，在这个实例里以threadlocal实例为键，存储对象。



> threadlocalMap解决哈希冲突的方式

处理冲突检测的机制是向后移位, 清除过期条目 最终找到合适的位置



> 为什么结束后要删除用户对象

会，当前线程结束后，当前线程对应的map应该被回收，map里的threadlocal，value都应该被回收，但是如果其它线程保持着对local实例的引用，当前对象的Map会被认定为存活对象，不能被回收，map里的threadlocal，value对也会一直存在。

解决办法就是在当前线程使用完local对象后，手动调用remove方法将其从Map中删除。



### 2、用elasticsearch做了什么，为什么要用，底层实现

> 做了什么

我希望能够对我的帖子进行一个搜素，就是我输入一些关键字，帖子包含这些关键字，或者部分关键字，就应该被搜索到。



> 为什么要用

一开始是使用mysql的like模糊查询做的搜索。

但是这样限制很多，首先是索引会失效，走全表扫描效率很低，再有就是不能做复杂的查询，比如我只需要包含关键词的一部分，模糊查询就做不到。

于是想到使用全文索引来做这个功能，mysql本身也支持全文索引，但是建立多个字段的全文索引会占用大量的磁盘空间，而且做复杂查询的时候会比较吃力。

es是专门做全文检索的，使用倒排索引来检索关键词，直接得到包含关键词的文档列表。它支持分布式的存储和搜索，将数据做了分片，存储到不同的结点；支持更高级的分词算法和匹配算法，在做复杂查询合海量数据查询的时候性能较好。



> es底层原理

主要是通过分区存储数据，以及通过分词算法切分关键词，然后通过倒排索引高校检索数据。



> 怎么做的搜索

首先配置好es服务器，将mysql的帖子数据存储到es中，并且在数据发生增删改的时候，同步到es服务器。

之后就是构建一个查询请求，向es发送查询请求，获取搜索的结果。



> 数据同步是怎么做的

mysql到es的数据同步方式有几种。

第一种是双写，有同步和异步的方式，同步就是在修改时同时写入es，异步就是发送到mq进行消费。双写策略需要耦合业务代码，而且需要自己处理同步失败的情况。

第二种是基于binlog订阅，binlog就是用来做主从同步的，通过第三方组件模拟为从节点，对mysql发起同步请求，然后将binlog回放，写到es。比如Canal，这种方式比较简单省事，而且海量数据同步效率也高。

第三种是基于sql查询做的数据抽取，通过第三方组件定时查询mysql，找到增量修改的数据，同步到es。比如logstash，这种方式需要增加修改时间的字段，ogstash会保存每一次查询的最后修改时间，通过实践判断哪个记录是新增的。



我使用的是canal做的数据同步。首先是开启mysql的binlog功能，然后安装canal服务器，配置数据库的连接，以及es客户端的连接参数，以及es和mysql表的映射关系。之后保持canal的运行就能实现数据同步。





> 查询请求是怎么样构建的

首先得执行GTE请求，然后是需要查询的索引，然后构建一个query请求体，在请求体中配置查询的模式，比如match，或者term，然后配置查询的字段以及响应的关键词，最后就是一些条件的配置，比如分组，排序，过滤，分页等等。





> es是怎么检索一个关键词的

解析查询请求：Elasticsearch会解析接收到的搜索请求，提取其中的索引名称、查询条件、排序规则等信息。

查询路由和分片选择：根据索引名称和查询条件，Elasticsearch确定要在哪些分片上执行查询操作。如果查询请求没有指定特定的分片，则会在索引中的所有分片上执行查询。

倒排索引搜索：Elasticsearch使用倒排索引来加速搜索过程。倒排索引是根据每个唯一词项（terms）建立的数据结构，记录了该词项在哪些文档中出现。根据查询条件中的关键词，在倒排索引中找到匹配的词项，进而找到对应的文档。

结果打分和排序：Elasticsearch对于每个匹配的文档进行打分，根据打分结果对文档进行排序。默认情况下，Elasticsearch使用TF-IDF（词频-逆向文档频率）算法来计算文档的相关性得分，也可以根据需要使用其他自定义的打分算法。

结果返回：根据用户的设定，Elasticsearch将搜索结果返回给用户。搜索结果通常包括匹配的文档、相关性得分以及任何附加的聚合结果。



> 倒排索引的原理是什么

倒排索引可以实现从词语到文档的映射。它由两部分组成：词典和倒排列表。

倒排索引将文档集合中的每个词语与包含该词语的文档列表关联起来。

词典是由所有不重复的词语构成的数据结构，用于存储词语及其对应的倒排列表的位置信息。

倒排列表则记录了包含该词语的文档的详细信息，如文档的编号、出现的位置等。



> 分词的原理是什么

分词器就是把一个短语通过一些分词算法拆分成语义独立的词语的一个组件。这些词语将作为倒排索引的字典。

分词器除了进行分词的操作，还会有一些别的处理，比如对分词后的词语做一个过滤，或者一些大写小写转化，以及更进一步的分词处理。

常见的分词算法有几种类型，一种是基于字典的，对短语做匹配，匹配成功后作为一个词语，并接着进行匹配。

也可以基于语法规则对短语做切分，根据语法，语义，词性等规则做切分。也可以基于深度学习算法，根据大量的数据训练来学习如何做拆分。



> es是怎么存储一条记录的

首先根据路由策略确定该数据应该路由到索引的哪个分片中。路由策略可以自定义，或者根据某个字段的值做区分。

把数据写入分片的缓冲区中，生成文档ID等信息以及倒排索引等数据，然后定期从缓冲区把数据刷到磁盘。



> term和match的区别

Term 查询是一种精确匹配的查询，它在搜索时不会对关键词进行分词处理。

Match 查询： Match 查询是一种通过分词来进行全文搜索的查询方式。它会对搜索条件进行分词，并将分词后的词项与索引中的词项进行匹配。

 

### 3、用Kafka做了什么，为什么要用，底层实现

> 做了什么

当发生点赞，关注和评论事件的时候，需要给关联的用户发送一条系统通知。我使用了异步执行的机制，就是先将这个事件发送给Kafka，再进行消费，把通知写入到数据库。

在注册账号的时候，有一个邮箱激活的需求，需要给指定邮箱发送一个激活邮件，这个发送的操作非常的耗时，会导致前端界面一直在转圈圈，所以我把这个发的动作发送到了Kafka，异步的执行发邮件的动作。



> 为什么用Kafka

Kafka可以处理大规模的实时数据流，拥有非常高的吞吐量和低延迟。

Kafka将所有消息持久化到磁盘上，因此不会因为消费者的速度慢或者宕机导致数据丢失，提高数据可靠性。

Kafka采用分布式架构，可以在多个服务器上进行水平扩展。将topic的数据存储到不同结点的分区上。而且还建立了副本存储数据，不会因为结点的宕机产生数据丢失的问题。

Kafka支持发布订阅模式，消费者可以订阅感兴趣的主题并实时接收消息，多个消费者组可以并行的消费同一个topic的不同分区。



> Kafka分区数量和消费者组怎么考虑

Kafka分区数量默认是1，可以在创建topic的时候指定分区数量，分区数量应该大于消费者组的数量，否则消费者会出现分不到分区消费，浪费资源的情况。多个消费者组之间是可以并行消费一个topic的。



> kafka底层原理、为什么吞吐量高

存储：Kafka使用持久化日志的方式来存储消息，基于顺序写和随机读的特性，以及磁盘顺序读写的高效性

副本：Kafka采用分布式的多副本机制，将主题的分区数据复制到多个Broker上。这样的设计不仅提高了数据的可靠性，还允许通过增加Broker节点来水平扩展整个系统的吞吐量和容量。

批量：Kafka支持批量处理消息，即生产者可以将多条消息批量发送到Broker。这种批量处理减少了网络开销和磁盘IO，提高了生产者和消费者的效率。同时，Kafka还支持消息的压缩，可以减小网络传输的数据量，节省带宽和存储空间。

消费：Kafka引入了消费者组的概念，组内的消费者可以同时处理多个分区的消息，实现了消息的并行处理和负载均衡。

Kafka提供了灵活的分区分配策略，确保分区在消费者组内均匀分布，以及在消费者加入或退出时自动进行重新分配。



> kafka消息丢失怎么解决

Kafka的消息丢失问题可能会发生在Broker、Producer和Consumer三种中的任意一种。

Broker消息丢失是因为采用异步批量的刷盘策略，宕机会丢失一部分数据，解决的办法就是同步刷盘。

producer消息丢失是因为采用异步发送的方式，没等Kafka确认就认为发送成功，解决办法就是改成同步发送，或者设置一定的重传次数。

consumer丢失数据是因为分区offset的提交是在消费之前，提交后没来得及消费就宕机了，解决办法就是改成消费消息之后再提交offset。



> kafka重复消费怎么解决

重复消费是由于offset提交的时候宕机了，Kafka把分区交给其它消费者进行消费。



> kafka顺序消费怎么实现

kafka每个partition中的消息在写入时都是有序的，消费时，每个partition只能被每一个group中的一个消费者消费，保证了消费时也是有序的。

整个topic不保证有序。如果为了保证topic整个有序，那么将partition调整为1.







### 4、用caffeine做了什么，为什么要用，底层实现

> 做了什么

我在首页会展示一页的帖子，在操作的过程中，会频繁的访问首页，需要不断的查询数据库，帖子的信息是很少会改变的，所以我想使用一个本地缓存来保存首页展示的帖子。直接从缓存中获取，不需要每次都查库。



> 为什么用caffeine

直接使用map也能实现简单的缓存效果，但是得单独做过期时间，过期之后再次访问还要重新加载，而且在并发环境下加载，有可能会出现线程安全的问题。

caffeine可以配置过期时间和过期策略，可以使用手动或者异步的方式加载数据，并且使用了分段锁保证并发环境下的线程安全问题，而且caffeine还提供了一些监控和统计的功能。

如果数据已经过期，会从redis重新加载数据，redis作为一个集中缓存，可以解决分布式共享的问题。



> caffeine底层是怎么实现的

Caffeine 使用了一种类似于哈希表的数据结构。它是一个线程安全的哈希表，采用分段锁的方式来提高并发性能。每个 不同的段可以被不同的线程同时访问，增加了并发度。

使用堆外内存来存储缓存数据，自己实现内存池，内存管理器来做内存分配和释放，减轻 Java 堆内存的压力，提高 GC 性能，并且通过跳表等数据结构来实现缓存数据的排序等操作。



> 堆外内存有什么不同

堆外内存绕过JVM直接使用操作系统提供的内存空间，不受JVM堆内存管理。

可以提供更大的内存空间，访问速度快，不受JVM的影响

但是需要自己管理内存的分配和释放等问题。





### 5、用redis做了什么，为什么要用，底层实现

作为数据库存储了点赞，关注信息，以及DAU和UV的统计信息，以及登录的时候保存登录状态信息还有验证码信息等，作为一个临时的缓存。



> 做了什么

为了缓解mysql数据库的压力，我把一些操作频繁，数据量小的数据放到了redis中，redis是一个性能高的，读取非常快的一个缓存，比较适合存储一些热点数据。



第一个是点赞信息，关注信息，这些信息的辩护频率非常的快。

第二个是UV和DAU统计信息，redis对于做统计有非常优秀的数据结构，可以很方便的进行数据统计。

第三个是一些会话状态的信息，比如一些用于验证身份的验证key，临时生成的验证码。这些信息保存到redis是为了解决分布式的一个问题，可以在分布式环境下共享状态信息，不会出现在这个结点登录之后，再次访问另一个结点获取不到的情况。

第四个是文章热度信息的保存，文章的热度是通过一个公式算出来的一个数字。它是需要频繁去更新的，所以把它放到了redis去保存。

第五个就是热点文章的数据缓存，给热点文章做缓存可以降低数据库的压力，提高响应速度。redis作为一个分布式环境的共享缓存，使用caffeine在本地做了高速的缓存，caffeine数据过期之后会从redis重新加载。因为我已经把文章变化频繁的数据做了抽离，而且文章被修改修改的频率是很低的，文章一致性问题带来的影响也不会很大。文章缓存在过期之前是不会和数据库数据做同步的，哪怕文章发生了修改，热榜发生了变化。在过期之后，才会去查热榜，得到热榜前一页文章的id，去访问数据库，然后把查询到的数据更新到redis。





> 用了哪些数据结构去保存

会话状态信息和热度信息，热点数据都使用简单的string类型，通过一个键值对的简单形式保存数据；

关注信息使用了sorted set，因为关注我除了要记录关注者，被关注者，还要记一个关注的时间，而且对这个时间有一个排序的需求，就是展示粉丝列表的时候，会根据这个关注时间排序展示。恰巧这个sorted set本身有一个分数，可以用来存时间，也很方便的做排序。

点赞信息使用了set，因为我对这个点赞没有做这个时间记录，所以只需要set记录就行，使用set可以方便的进行排除重复点赞。

UV和DAU的记录使用了HyperLogLog统计，HyperLogLog使用了基数统计的算法，使用固定数量的内存来估计集合的基数，计算速度快，空间占用小。



> 这些数据结构的底层有了解过吗

string底层是一个动态数组sds，使用字节数组来存储数据，通过空间预分配，以及扩容可以实现动态扩展。string的字节数组是以二进制的格式存储的，所以能存储任何形式的数据，包括字符，图片，视频等。string实现了一系列方便操作字符串的api，比如替换，截取，追加等等，功能非常的强大。



set底层是用哈希表实现的，哈希表是一种键值对的数据结构，其中每个元素由一个唯一的键和对应的值组成。set中的每一个元素是哈希表的键，哈希表的值是忽略的。



Sorted Set底层的实现主要基于跳表（Skip List）和哈希表两个数据结构，跳表由多个层级组成的，每个层级都是一个有序链表，可以实现向后和向下查找，就类似与二叉树的结构，可以实现ln(n)效率的查找。哈希表是用来存储元素和分数对应关系的。



Hyperloglog底层是基于一定数量的桶作为计数器，通过一个哈希函数将元素转变为固定长度的哈希值，通过哈希值前缀定位到属于它的桶，然后，桶会记录哈希值最长连续尾部0的最大个数。这个最大个数后边经过求平均数之后作为一个基数，去估计元素的数量。基数估计的核心原理是通过桶内的最大连续尾部零个数来推断数据集的基数大小。





> 分布式锁有了解吗

在分布式环境中，会有多个结点竞争共享资源资源，不可避免地会出现并发安全问题，本地锁只能作用在同一台机器上，它是基于操作系统实现的，不能协调控制到其它的结点。所以需要一个作用范围更大的，能在多台机器共享的锁，来保证分布式并发操作的安全性。



分布式锁的实现原理也很简单，就是多个结点去获取同一把锁，保证只有一个结点获取成功，并且处理好死锁问题，以及锁的可靠性问题即可。



可以使用数据库的行锁或者利用数据库做一个乐观锁的机制去保证只有一个结点能够获取到锁。也可以使用redis单线程的安全性去保证只有一个结点能够获取成功。在redis使用set nx命令可以设置一个锁，nx命令保证了如果key为空才会设置成功，再加上redis是单线程的，就能保证只有一个结点能够得到锁。



死锁问题是因为结点宕机，不释放锁造成的，解决办法就是给key加一个过期时间。如果过期了，redis或者其它线程就能够释放并获取锁。但是结点有可能没有宕机，只是执行时间久，所以还得解决锁的误删问题。使用心跳机制可以解决这个问题，锁快过期了就发送请求给锁的结点，如果它还在使用，就为这个锁延长过期时间。



最后一个问题就是锁的可能性问题，redis结点可能会宕机，导致锁服务不可用，所以需要一个集群环境。主从环境的模式可能会出现脑裂的情况，就是从节点上位后给其它结点上锁，然后主节点恢复，导致锁被多个结点获取到。

可以使用Redlock算法解决这个问题，就是设置多个相互独立的主节点，当结点尝试获取锁的时候，向所有的主节点发送加锁请求，只有超过一半的结点加锁成功，才认为分布式锁加锁成功，否则会回退，在已经加锁成功的redis结点释放锁。



> redis为什么这么快

第一个原因是redis把数据存储到了内存，内存的读写速度是远远超过磁盘的。

第二个是redis使用单线程模型，避免了上下文切换以及并发控制带来的开销。

还有就是redis向操作系统提交请求是异步的，在等待提交结果的时候不会阻塞其它请求提交。

还有一个比较重要的原因是redis使用简单高效的数据结构，很多都是基于哈希表的存储，查找复杂度是o(1)

在网络通信方面没有使用http协议，而是使用自己开发的resp协议，resp是基于二进制的协议，数据格式非常简单，可以快速进行解析和序列化，降低网络传输开销。





> 有用过redis事务吗

在做点赞的时候，除了要记录文章的帖子数量变化，同时对于文章作者总共获得的点赞数，也要同时变化。还有关注的时候，除了被关注者的粉丝列表发送变化，关注者的关注列表也要同时变化。这就需要使用redis事务来保证一致性。



redis事务使用multi开启，使用exec执行，使用watch监控。redis事务保证在开启时候后，执行事务之前，如果发生了key的变化，或者命令出现语法错误，会发生回滚，不执行任何命令。

但是一旦事务开始执行了，发生运行时错误，redis事务不会回滚，其它命令正常执行。

redis还保证在事务的命令之间不会插入其它客户端的命令，保证了事务的一致性。





### 6、用线程池做了什么，线程池有哪些参数

查找所有帖子的时候，使用线程池并发的查找每一个帖子关联的评论，使用concurrentMap聚合起来。



> 做了什么

在查询一个帖子的详情信息的时候，会查询它的评论信息，而且每一个评论还需要去查询评论的人的信息，这条评论的点赞信息，以及回复信息每一条评论都要进行相同的查询，彼此独立没有交互，所以使用线程池去分别查询每一个评论的回复信息吗，提高查询的效率。使用threadpoolExecutor创建一个线程池，配置了它的一些参数，比如核心线程数，最大线程数，阻塞队列长度，拒绝策略。使用countdownlatch阻塞主线程，等待所有查询结束。因为每一个评论都是放到一个map里边的，它的用户信息，点赞信息，回复信息也都是放到这个map里边，不同的评论有不同的map，不会发生并发修改的问题。



> 说一下线程池的几个参数





> 





### 7、用Quartz做了什么，为什么要用，底层实现

帖子热度计算，不能点赞一个算一次，而是定时去计算，更新。

也不是每次都更新所有的帖子，有事件发生的帖子才需要更新



配置化开发，可以配置作业的参数、并发性、持久化等属性，支持支持在集群环境下进行作业调度







### 8、使用springsecurity做了什么，为什么要用，底层实现







### 9、使用docker做了什么，为什么要用，底层实现







### 10、有没有用到事务，为什么要用，底层实现

对一些有写操作的service方法。



redis关注时使用事务



> spring怎么实现事务





> mysql怎么实现事务





### 11、进行了哪些JVM参数的调整







